{
 "cells": [
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": [
    "!pip install contractions"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Collecting contractions\n  Downloading contractions-0.0.45-py2.py3-none-any.whl (6.4 kB)\nCollecting textsearch\n  Downloading textsearch-0.0.17-py2.py3-none-any.whl (7.5 kB)\nRequirement already satisfied: Unidecode in /opt/conda/lib/python3.7/site-packages (from textsearch->contractions) (1.1.1)\nCollecting pyahocorasick\n  Downloading pyahocorasick-1.4.0.tar.gz (312 kB)\n\u001B[K     |████████████████████████████████| 312 kB 820 kB/s eta 0:00:01\n\u001B[?25hBuilding wheels for collected packages: pyahocorasick\n  Building wheel for pyahocorasick (setup.py) ... \u001B[?25ldone\n\u001B[?25h  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.0-cp37-cp37m-linux_x86_64.whl size=99073 sha256=e267dda6b2381f5b962367e074e2ca1d7217e6611bcb6dec84b269d8d2fe83ba\n  Stored in directory: /root/.cache/pip/wheels/9b/6b/f7/62dc8caf183b125107209c014e78c340a0b4b7b392c23c2db4\nSuccessfully built pyahocorasick\nInstalling collected packages: pyahocorasick, textsearch, contractions\nSuccessfully installed contractions-0.0.45 pyahocorasick-1.4.0 textsearch-0.0.17\n\u001B[33mWARNING: You are using pip version 20.3.1; however, version 21.0 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001B[0m\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Importación de las librerias."
   ]
  },
  {
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Libreria equivalente a numpy optimizada para CUDA\nimport cupy as cp\nimport cupyx\n\n# Libreria para el procesamiento de lenguaje natural NLP\nimport spacy\n\n# Libreria para dividir un dataset en train y test\nfrom sklearn.model_selection import train_test_split\n\n# Permite vectorizar un text corpus (transforma texto a una secuencia de enteros)\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# Transforma una lista de secuencia de enteros a una lista normalizada.\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Permite trabajar con contracciones de palabras, por ejemplo Don't, We're, etc.\nimport contractions\n\n# Permite normalizar cadenas de texto eliminando caracteres como tildes.\nimport unidecode\n\n# Libreria para utilizar el algoritmo Naive Bayes a traves de CUDA.\nfrom cuml.naive_bayes import MultinomialNB\n\n# Libreria para trabajar con Regex\nimport re\n\n# Libreria para el manejo de matrices y manejo de datos avanzados.\nimport pandas as pd\n\n# Libreria enforcada en el manejo de matrices.\nimport numpy as np\n\n# Libreria para medir el tiempo.\nimport time",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2. Lectura de los 3 datasets de diferentes fuentes."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Permite acceder a funciones del Sistema Operativo.\nimport os\n\n# Leemos cada uno de los datasets con los que vamos a trabajar.\ndataframe_yelp = pd.read_csv('/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/yelp_labelled.txt', delimiter='\\t', names=['text', 'sentimiento'])\ndataframe_amazon = pd.read_csv('/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/amazon_cells_labelled.txt', delimiter='\\t', names=['text', 'sentimiento'])\ndataframe_imdb = pd.read_csv('/kaggle/input/sentiment-labelled-sentences-data-set/sentiment labelled sentences/imdb_labelled.txt', delimiter='\\t', names=['text', 'sentimiento'])\n\n\n# Concatenamos los 3 datasets.\ndata = pd.concat([dataframe_yelp, dataframe_amazon, dataframe_imdb])\n",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 Imprimimos la cabecera de los 3 datasets previamente concatenados."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "data.head()",
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "execution_count": 5,
     "data": {
      "text/plain": "                                                text  sentimiento\n0                           Wow... Loved this place.            1\n1                                 Crust is not good.            0\n2          Not tasty and the texture was just nasty.            0\n3  Stopped by during the late May bank holiday of...            1\n4  The selection on the menu was great and so wer...            1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentimiento</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Wow... Loved this place.</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Crust is not good.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Not tasty and the texture was just nasty.</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Stopped by during the late May bank holiday of...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The selection on the menu was great and so wer...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Splitting de Train y Test"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Se establece los pandas.Series del train y test, en la cual 70% del dataset va para el train y 30% del dataset se usa en el test.\n\nx_train, x_test, y_train, y_test = train_test_split(data['text'], data['sentimiento'], test_size = 0.30, random_state=42)\nvalue_test = x_test[0:20]\nexpected_test = y_test[0:20]",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4. Creacion de las funciones para el preprocesamiento de datos."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "\"\"\"\n    Permite eliminar caracteres especiales que no sean letras y numeros.\n\"\"\"\ndef quitar_caracteres_especiales(data):\n    pattern = r'[^a-zA-z0-9\\s]'\n    for i in range(len(data)):\n        data[i] = re.sub(pattern, '', data[i])\n    return data\n    \n\n\"\"\"\n    Quita los acentos de palabras, por ejemplo caffé pasaria a ser cafe.\n\"\"\"\ndef quitar_acentos(data):\n    for i in range(len(data)):\n        data[i] = unidecode.unidecode(data[i])\n    return data\n\n\"\"\"\n    Permite expandir las contracciones de palabras como Don't a Do not o We're a we are.\n\"\"\"\ndef expandir_contracciones(data):\n    for i in range(len(data)):\n        data[i] = contractions.fix(data[i])\n    return data\n\n\n\"\"\"\n    Lematiza las palabras, por ejemplo de Jumping a Jump, o Playing a Play, o Bought a Buy.\n\"\"\"\ndef lematizar_palabras(data):\n    nlp = spacy.load('en_core_web_lg')\n    for i in range(len(data)):\n        doc = nlp(data[i])\n        tokens = [word.lemma_ if word.lemma_ != \"-PRON-\" else word.lower_ for word in doc]\n        data[i] = ' '.join(tokens)\n    return data\n\n\"\"\"\n    Esta funcion se encarga de llamar a las funcionea antes definidas para procesar al dataset entero.\n\"\"\"\n\ndef preprocesar(data):\n    data = data.to_numpy()\n    \n    data = quitar_caracteres_especiales(data)\n    data = quitar_acentos(data)\n    data = expandir_contracciones(data)\n    data = lematizar_palabras(data)\n    \n    data = pd.Series(data)\n    \n    # Realiza el proceso de la tokenizacion de cadenas de texto.\n    tokenizer = Tokenizer(num_words=250, lower= 1, oov_token=\"<OOV>\")\n    tokenizer.fit_on_texts(data)\n    sequences = tokenizer.texts_to_sequences(data)\n    \n    # Permite llenar una secuencia de enteros con 0 para igualar su tamano.\n    padded_data = pad_sequences(sequences, padding='post', maxlen=350)\n    return padded_data",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5. Preprocesamiento de datos aplicado al Train y Test"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Preprocesamos nuestros pandas.Series, tanto en train como test.\nx_train = preprocesar(x_train)\nx_test = preprocesar(x_test)",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 6. Entrenamiento de nuestro clasificador Naive Bayes"
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Pasamos nuestro np.array a un cupix array de enteros (Nos permite trabajar con CUDA).\ny_train = cp.asarray(y_train, dtype=cp.int32)\n\n# Instanciomos a nuestro clasificador Naive Bayes.\nmodel = MultinomialNB()\n\n# Mandamos a entrenar a nuestro clasificador.\ntiempo_inicial = time.time()\nmodel.fit(x_train, y_train)\n\nprint('Accuracy del clasificador: ', model.score(x_train, y_train))\ntiempo_final = time.time()-tiempo_inicial\nprint('Tiempo aproximado consumido por el algoritmo (en nanosegundos): ', tiempo_final)\n",
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Accuracy del clasificador:  0.5538221597671509\nTiempo aproximado consumido por el algoritmo (en nanosegundos):  0.012101173400878906\n",
     "name": "stdout"
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 7.Evaluacion del accuracy de nuestro clasificador NB."
  },
  {
   "metadata": {
    "trusted": true
   },
   "cell_type": "code",
   "source": "# Utilizamos 20 oraciones al azar de nuestro conjunto de frases para Test.\nx_test = x_test[0:20]\n\n# Predecimos nuestro conjunto de test.\nprediccion = model.predict(x_test)\n\n# Transformamos nuestros resultados a np.array.\nvalue_test = value_test.to_numpy()\nexpected_test = expected_test.to_numpy()\n\n# Imprimos los resultados del entrenamiento.\nfor i in range(20):\n    print('Frase: ', value_test[i], '\\n', '--'*50, '\\n', 'sentimiento predicho: ', prediccion[i], ', valor esperado: ', expected_test[i])\n    print('__.__'*25)",
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": "Frase:  the attractive set use throughout most of the film be an eyeplease gem   \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  1 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  this be one of the bad film i have ever see   \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  the microphone also work well but accord to people i have call it applifie everything \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:    come with a strong light that you can use to light up your camera shot and even flash so be signal seriously \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  1 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  the acting be like watch wooden puppet move around and read from a book that be how bad it be   \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  generous portion and great taste \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  very comfortable \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  1 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  he be almost unbearable to watch on screen he have little to no charisma and terrible comedic timing   \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  go for lunch   service be slow \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  1 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  also the fry be without a doubt the bad fry i have ever have \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  the refried bean that come with my meal be dry out and crusty and the food be bland \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  product be excellent and work well than the verizon one and Boy be it cheap \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  bland and flavorless be a good way of describe the barely tepid meat \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  1 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  service be exceptional and food be a good as all the review \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  i pay too much   \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  1 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  the budget be evidently very limited   \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  be sure to order dessert even if you need to pack it togo   the tiramisu and cannoli be both to die for \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  1 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  it always cut out and make a beep beep beep sound then say signal fail \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  everyone be very attentive provide excellent customer service \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  1 , valor esperado:  1\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\nFrase:  its so stupid to have to keep buy new charger car charger cradle headphone and car kit every time a new phone come out \n ---------------------------------------------------------------------------------------------------- \n sentimiento predicho:  0 , valor esperado:  0\n__.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.____.__\n",
     "name": "stdout"
    }
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}